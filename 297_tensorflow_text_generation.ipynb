{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChirudeepG/Transformers-and-finetuning-with-LLMs/blob/main/297_tensorflow_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1S2Pfej68yQ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "nmPiey8O7nM5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/S01E07 The Blackout.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "MKTyEOoS7kct"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kwPrUs56-0fY"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoding the data\n",
        "data = [stoi[c] for c in text]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "\n",
        "train_data_tensor = tf.constant(data[:n], dtype=tf.int32)\n",
        "val_data_tensor = tf.constant(data[n:], dtype=tf.int32)"
      ],
      "metadata": {
        "id": "8LisTMDp8ywx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data_tensor, batch_size, block_size):\n",
        "    start_indices = tf.random.uniform((batch_size,), 0, len(data_tensor) - block_size, dtype=tf.int64)\n",
        "    x_batch = tf.stack([data_tensor[start:start + block_size] for start in start_indices])\n",
        "    y_batch = tf.stack([data_tensor[start + 1:start + block_size + 1] for start in start_indices])\n",
        "    return x_batch, y_batch"
      ],
      "metadata": {
        "id": "hp1Plrb77htO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-0QIKaWc3MXs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.values = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.keys = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.queries = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.fc_out = layers.Dense(embed_size)\n",
        "\n",
        "    def call(self, values, keys, query):\n",
        "        N, seq_length, _ = query.shape\n",
        "        value_len, key_len = values.shape[1], keys.shape[1]\n",
        "\n",
        "        # Split embedding into self.head pieces\n",
        "        values = tf.reshape(values, (N, value_len, self.heads, self.head_dim))\n",
        "        keys = tf.reshape(keys, (N, key_len, self.heads, self.head_dim))\n",
        "        queries = tf.reshape(query, (N, seq_length, self.heads, self.head_dim))\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = tf.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
        "        attention = attention / tf.math.sqrt(float(self.head_dim))\n",
        "        attention = tf.nn.softmax(attention, axis=-1)\n",
        "\n",
        "        out = tf.einsum(\"nhql,nlhd->nqhd\", attention, values)\n",
        "        out = tf.reshape(out, (N, seq_length, self.embed_size))\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.feed_forward = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(forward_expansion * embed_size, activation=\"relu\"),\n",
        "                layers.Dense(embed_size),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, value, key, query):\n",
        "        attention = self.attention(value, key, query)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(keras.Model):\n",
        "    def __init__(self, vocab_size, embed_size, heads, n_layers, max_length, forward_expansion, dropout):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embedding = layers.Embedding(max_length, embed_size)\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.fc_out = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        N, seq_length = x.shape\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        out = self.embedding(x)\n",
        "        out += self.positional_embedding(positions)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            out = block(out, out, out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "heO-pJIE7XjV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(\n",
        "    vocab_size,\n",
        "    n_embd,\n",
        "    n_head,\n",
        "    n_layer,\n",
        "    block_size,\n",
        "    forward_expansion=n_embd * 4,\n",
        "    dropout=dropout\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "generated_text = []\n",
        "for iteration in range(max_iters):\n",
        "    x_batch, y_batch = get_batch(train_data_tensor, batch_size, block_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if iteration % eval_interval == 0:\n",
        "        print(f\"Iteration {iteration}, Loss: {loss.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Dfqd4i9C3c",
        "outputId": "2b982196-87c9-4af1-cbc1-09802935cfa6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7cc3fb97ce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7cc3fb97ce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.995068073272705\n",
            "Iteration 100, Loss: 3.3651232719421387\n",
            "Iteration 200, Loss: 3.32047438621521\n",
            "Iteration 300, Loss: 3.290300130844116\n",
            "Iteration 400, Loss: 3.366560220718384\n",
            "Iteration 500, Loss: 3.29370379447937\n",
            "Iteration 600, Loss: 3.3987698554992676\n",
            "Iteration 700, Loss: 3.275597333908081\n",
            "Iteration 800, Loss: 3.3413777351379395\n",
            "Iteration 900, Loss: 3.042710542678833\n",
            "Iteration 1000, Loss: 2.942659854888916\n",
            "Iteration 1100, Loss: 2.6498396396636963\n",
            "Iteration 1200, Loss: 2.543227195739746\n",
            "Iteration 1300, Loss: 2.6203181743621826\n",
            "Iteration 1400, Loss: 2.636044502258301\n",
            "Iteration 1500, Loss: 2.4142119884490967\n",
            "Iteration 1600, Loss: 2.5263314247131348\n",
            "Iteration 1700, Loss: 2.2889983654022217\n",
            "Iteration 1800, Loss: 2.366955280303955\n",
            "Iteration 1900, Loss: 2.380937337875366\n",
            "Iteration 2000, Loss: 2.32063889503479\n",
            "Iteration 2100, Loss: 2.405618190765381\n",
            "Iteration 2200, Loss: 2.21152663230896\n",
            "Iteration 2300, Loss: 2.2271673679351807\n",
            "Iteration 2400, Loss: 2.429385185241699\n",
            "Iteration 2500, Loss: 2.296041488647461\n",
            "Iteration 2600, Loss: 2.238987684249878\n",
            "Iteration 2700, Loss: 2.1780641078948975\n",
            "Iteration 2800, Loss: 2.231391429901123\n",
            "Iteration 2900, Loss: 2.1054024696350098\n",
            "Iteration 3000, Loss: 2.101249933242798\n",
            "Iteration 3100, Loss: 2.0890064239501953\n",
            "Iteration 3200, Loss: 2.104501962661743\n",
            "Iteration 3300, Loss: 2.1524484157562256\n",
            "Iteration 3400, Loss: 2.0241148471832275\n",
            "Iteration 3500, Loss: 1.805132508277893\n",
            "Iteration 3600, Loss: 1.9534683227539062\n",
            "Iteration 3700, Loss: 1.854060173034668\n",
            "Iteration 3800, Loss: 1.760788083076477\n",
            "Iteration 3900, Loss: 1.7583616971969604\n",
            "Iteration 4000, Loss: 1.8604987859725952\n",
            "Iteration 4100, Loss: 1.8733242750167847\n",
            "Iteration 4200, Loss: 1.6367502212524414\n",
            "Iteration 4300, Loss: 1.8819489479064941\n",
            "Iteration 4400, Loss: 1.6345382928848267\n",
            "Iteration 4500, Loss: 1.8631049394607544\n",
            "Iteration 4600, Loss: 1.6788607835769653\n",
            "Iteration 4700, Loss: 1.6624870300292969\n",
            "Iteration 4800, Loss: 1.5696675777435303\n",
            "Iteration 4900, Loss: 1.5944135189056396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, max_generate_length=2000):\n",
        "    # Convert start_string to tensor\n",
        "    input_eval = [stoi[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    generated_text = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(max_generate_length):\n",
        "        logits = model(input_eval)\n",
        "        # Use a multinomial distribution to predict the token returned by the model\n",
        "        predicted_id = tf.random.categorical(logits[:, 0, :], num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "        # Append the predicted token to the input string and the generated text\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        generated_text.append(itos[predicted_id])\n",
        "\n",
        "    return ''.join(generated_text)\n",
        "\n",
        "start_string = \" \"  # You can use a space, or any other starting token\n",
        "print(generate_text(model, start_string))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjT-NwcRht-N",
        "outputId": "7459091b-3f97-47f5-d4a5-9c64468434cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aq\n",
            " ' qsa qAAnACNA.,.....gtc\n",
            " aaI baqACCNCNWvnCNwCnCnARCNpWWcRAWP\n",
            " i..  a\n",
            " hc?     sc\n",
            " gc?]....  gWvnCsyCCnCCnNWc\n",
            " gggttgttcBCCnCnGM\n",
            "  ,... nYNCCnOcynCni.....,   ghOMPynnAnCCCnBTYYWMyCCT\n",
            "c?R...... \n",
            "aonCnCnCNCnCnCnCnAnBCnNcPe  hWM\n",
            "   ac!lb'WWcynCnCnNAnAnvnA..  nNJWWWc\n",
            " ugLbBWc?qCNunCNCnCNAttgtgWWcyNACCNAm\n",
            "   tthCFnCnNCCCT\n",
            "  qRAnCnCnAnNwAnNcWMc?Anttc?nNAnCnAAnNOcyCnAnnCNCCnCCCNCnAnCnCtc?W[\n",
            " qR........ hAiiDcaI.,....  nOc!\n",
            ".. gWMc?Rc\n",
            " nCnAR..,.   gtc?WMc!rtc?aetaqa gttcyCnNwwCCnOtttc\n",
            " unAnCnnwCCnY:sSgc\n",
            "... qAnCnpwcc?fii c\n",
            "hc...'... qAnbc?......  \n",
            "lbnCnA.   cAnCnCCNcypCCnCNAnNCCnCnCnwdddrc!Anii   nCNCCCnCnAnCpvNcc?\n",
            "    gttgcbnAnwnCnCNCnATcBCnjnmTCNYCnAnNo\n",
            "  qAnCnCCnYFnCnCnCCN  ggcyTMpTjjnCnNA...,...   q\n",
            "WM?\n",
            " ahAlsJAp:.....,.  e nCnCnAnGWWWMWvCNCCnQtggggM cP\n",
            " gtttc!\n",
            "  nO\n",
            "nCCnCnCnCnNAbc?qAnJ:l  aqRR. a anCnCnCnAwo nCnAtttc...... lddddddrc\n",
            "u   a  tc?fqRqR..,.   onNCCpCCCnNAnCnCnCnnCnCnGOcyNNbn(pWTO\n",
            "a\n",
            "o c?[R..,.etc?RCn\n",
            " nCnA..,......  aa\n",
            "c\n",
            "aqAntt nCCnCNCnNc?\n",
            "hTACnBbc!J\n",
            " qRc?.,. ggt. acyCCNCCnBCnCnwwAnWM\n",
            " nCnQnATJA a nAnJA.... gc?\n",
            " c\n",
            " a\n",
            " nAwAnwo nw  qsRyCNOMcyCnCCNgCnNATcyTCnCYCCNJ:AnAnCCOcRR.sc?i   c?AnCnCnAnCAnCnOcynAnNc?nnCCnCnCnYCCngWcc?nCnCNCCnAgggWcRCng\n",
            " aqRgcAAyNCn5imaa a  gce qRAnCnCNAnCCnCnBCCnNAnwOc?ttc\n",
            "nCnNCno'  g(pWvnNwAnCn3AnAnAnGGWc\n",
            "  'q\n",
            "qtc\n",
            " qStttc\n",
            "  anYCpAnCYTCCnGvCnAnCCnCCnNc!ACnAnCwCnCnOc ahMbc\n",
            "WMypOcypCnttWWc\n",
            "aWWMc?PynNc\n",
            " al  qCnNA c!sSgtttc?AnCCNCCn........ cR...............'  \n",
            "nJAn:\n",
            "  WvnGWMcsSa  'I.......,. ?CAnNCnnYAnAi 'nNCnQc\n",
            "R....... njttc\n",
            " qAAn\n",
            "\n",
            "hgcJWc\n",
            "c!PyNYECnNAnCCnWc!sUnAnNAnAi. c\n",
            "  syNA.... ubdd nYCnYTOMBCnAfR.........   'edd syCnGWMM\n",
            "gc?qCnCNAnNo\n",
            " emyNc?? o  qR. gcyCnCnNAw nAnCNCCCCNc?qANOce..... l  c\n",
            "  \n",
            "  ce ?\n",
            "\n",
            " aaLb cyCNcJOc?0ls9sc\n",
            "RJJ\n",
            "qACnCn(rtte cAmrtcynNcynCnC(AnCnNAnCCpCnNwAR..... afqyNje qANCNynw ulb?qAnOthYTwyNcyBCCNCnCCnOc\n",
            "qR..... hvnCnnCCCCCnWcc  o   I  ltc\n",
            "grtc\n",
            " aqAfi...........  nCNCNAnCnCCQtttt  aa\n",
            " oR..   f aed u qCAnCnCNOb  c\n",
            "c?AnCCNCnNWcyCnCnmwgWWc\n",
            "kk.... aqAnCnGI hvpCnNw\n",
            "hWc?WTMJCpPyNo'nN   WWHbnCnAnNAnAnCNAnCNCnCNAnNApOcJpg[ \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}